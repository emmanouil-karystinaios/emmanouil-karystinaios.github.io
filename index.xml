<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Emmanouil Karystinaios</title><link>https://emmanouil-karystinaios.github.io/</link><atom:link href="https://emmanouil-karystinaios.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Emmanouil Karystinaios</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate><image><url>https://emmanouil-karystinaios.github.io/media/icon_hu4c3aa08d28a737b1b7fdd38226539d61_369298_512x512_fill_lanczos_center_3.png</url><title>Emmanouil Karystinaios</title><link>https://emmanouil-karystinaios.github.io/</link></image><item><title>Example Talk</title><link>https://emmanouil-karystinaios.github.io/talk/example-talk/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/talk/example-talk/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.
&lt;/div>
&lt;/div>
&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Wowchemy&amp;rsquo;s &lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">page elements&lt;/a> such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>Language Models for Music Medicine Generation</title><link>https://emmanouil-karystinaios.github.io/post/musicmedicine/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/post/musicmedicine/</guid><description>&lt;p>Recent advances in generative AI has reached the music field.
Many generative music models can lead to awesome creations.
But there is one compelling new application that has been left until now unexplored, music therapy.
This research, originally presented at the ISMIR 2024 Conference as a late breaking demo, introduces a novel model for generating music for music therapy purposes, named &lt;em>Language Models for Music Medicine Generation&lt;/em>. The paper explores how AI-driven music can support mental health by guiding listeners through emotional transitions using generative music designed to align with established therapeutic principles.&lt;/p>
&lt;h3 id="background-the-therapeutic-power-of-music">Background: The Therapeutic Power of Music&lt;/h3>
&lt;p>Music therapy, recognized since the 19th century, has been scientifically shown to provide significant mental health benefits, such as reducing anxiety and improving mood. While traditional music therapy is conducted by certified professionals, &lt;em>Music Medicine&lt;/em> offers a new approach: self-administered music interventions that can be beneficial between therapist-guided sessions.&lt;/p>
&lt;h3 id="model-overview-fine-tuning-musicgen-for-therapy">Model Overview: Fine-Tuning MusicGen for Therapy&lt;/h3>
&lt;p>To create a therapeutic experience, the researchers fine-tuned MusicGen, a state-of-the-art generative model initially trained on vast amounts of instrumental music, using the MTG-Jamendo dataset annotated with emotion and mood labels. They adapted MusicGen to produce music that aligns with specific emotional states, using a method known as low-rank adaptation (LoRA), which enables efficient fine-tuning without altering the original model’s core weights.&lt;/p>
&lt;h3 id="the-iso-principle-and-emotional-transitions">The Iso Principle and Emotional Transitions&lt;/h3>
&lt;p>Central to this model is the &lt;em>iso principle&lt;/em>, a technique in music therapy where music reflecting the listener’s current emotional state gradually shifts to evoke a more desired emotional outcome. The model generates sequences of 30-second audio clips, each crafted to represent an intermediate emotional state between the listener&amp;rsquo;s current mood and their targeted emotional state. These clips are then combined to form a cohesive 15-minute &amp;ldquo;music medicine&amp;rdquo; session that smoothly transitions across a spectrum of emotions.&lt;/p>
&lt;h3 id="methodology-creating-emotional-continuity">Methodology: Creating Emotional Continuity&lt;/h3>
&lt;p>Each audio clip is generated through prompt engineering, where tags define the emotional state, preferred instrument, and genre. To create a seamless experience, the clips are normalized, trimmed, and then overlapped and crossfaded. The model&amp;rsquo;s parameters, such as genre and instrumentation, are adjusted to mirror the emotion distribution in the dataset, adding depth to the experience.&lt;/p>
&lt;h3 id="results-and-performance-evaluation">Results and Performance Evaluation&lt;/h3>
&lt;p>The team tested three versions of the fine-tuned model—MusicGen-Small, Medium, and Large—each exhibiting varying improvements over the base model in capturing mood. The model&amp;rsquo;s output was evaluated using metrics such as the Contrastive Language-Audio Pretraining (CLAP) score, which measures audio relevance to the intended emotion. Additionally, precision-recall and Hamming scores validated that the music aligned with targeted emotional states.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;p>This research demonstrates a new frontier in AI-assisted mental health care, with the potential for Music Medicine to serve as a bridge between formal therapy sessions. The team plans to conduct user studies to assess the model&amp;rsquo;s effectiveness and work on more fluid transitions between emotional states for an even more natural experience. As AI continues to evolve, such innovations point to a future where technology enhances well-being through immersive potentially multi-sensor, emotionally resonant interventions.&lt;/p>
&lt;h3 id="aknowledgements">Aknowledgements&lt;/h3>
&lt;p>This work, titled Language Models for Music Medicine Generation, was collaboratively developed by Emmanouil Nikolakakis, Joann Ching, Emmanouil Karystinaios, Gabrielle Sipin, Gerhard Widmer, and Razvan Marinescu.&lt;/p></description></item><item><title>Symbolic Music Analysis with Graph Neural Networks</title><link>https://emmanouil-karystinaios.github.io/publication/2024_thesis/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2024_thesis/</guid><description/></item><item><title>Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving</title><link>https://emmanouil-karystinaios.github.io/publication/2024_ismir_vocseppoly/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2024_ismir_vocseppoly/</guid><description/></item><item><title>GraphMuse: A Library for Symbolic Music Graph Processing</title><link>https://emmanouil-karystinaios.github.io/publication/2024_ismir_graphmuse/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2024_ismir_graphmuse/</guid><description/></item><item><title>From Notes to Insights - Visualizing Graph Neural Network Explanations with SMUG-Explain</title><link>https://emmanouil-karystinaios.github.io/post/smug/</link><pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/post/smug/</guid><description>&lt;p>Graph Neural Networks (GNNs) are making waves in the world of Music Information Research (MIR). From predicting cadences to generating expressive performances, these networks are unlocking new potentials in understanding and processing musical scores. But there’s a catch — their complex, “black-box” nature makes them hard to interpret. That’s where SMUG-Explain comes in. This innovative framework is designed to generate and visualize explanations for GNNs applied to musical scores. Let’s dive into how SMUG-Explain works, its application in cadence detection, and its potential to bring AI and musicology closer together.&lt;/p>
&lt;h2 id="the-challenge-understanding-gnns-in-music">The Challenge: Understanding GNNs in Music&lt;/h2>
&lt;p>GNNs are fantastic at capturing intricate relationships in graph-structured data, which is perfect for tasks like cadence detection and voice separation in music. However, they’re notoriously difficult to interpret. For musicians and researchers, this opacity can be a big problem. Imagine having a super-smart assistant who helps you analyze music but never explains why it made certain decisions. Frustrating, right?&lt;/p>
&lt;p>Enter SMUG-Explain, or Score MUsic Graph Explain. This is a framework for interpreting GNNs applied to music but it also integrates these explanations directly into the musical scores. By visualizing how each note and its features contribute to the model’s predictions, SMUG-Explain makes GNNs more transparent and understandable.&lt;/p>
&lt;h2 id="how-does-smug-explain-work">How Does SMUG-Explain Work?&lt;/h2>
&lt;h3 id="graph-based-representation-of-musical-scores">Graph-Based Representation of Musical Scores&lt;/h3>
&lt;p>SMUG-Explain transforms a musical score into a graph where notes are vertices, and edges represent their temporal relationships. It uses four types of edges:&lt;/p>
&lt;ul>
&lt;li>Onset edges: Connect notes that start together.&lt;/li>
&lt;li>Consecutive edges: Link notes where one ends as the next begins.&lt;/li>
&lt;li>During edges: Connect notes that occur within the duration of another note.&lt;/li>
&lt;li>Rest edges: Connect notes across rests.&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure id="figure-in-this-example-we-view-the-score-graph-on-an-excerpt-of-a-perfect-authentic-cadence-pac-with-harmonic-annotations-written-in-roman-numerals">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/Example_Score.png" alt="Score Graph Example" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
In this example, we view the score graph on an excerpt of a Perfect Authentic Cadence (PAC) with harmonic annotations written in Roman numerals.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="explanation-techniques">Explanation Techniques&lt;/h3>
&lt;p>SMUG-Explain employs several post-hoc, gradient-based explanation methods such as Saliency, Integrated Gradients, Deconvolution, and Guided Backpropagation. These techniques evaluate the importance of each note and its features in predicting specific musical events, such as cadences. Additionally, these methods allow us to derive attribution weights for every edge in the input graph. By selecting the top-k most important edges, we can construct an explanation subgraph that highlights the critical components influencing the model’s predictions.&lt;/p>
&lt;h3 id="evaluation-of-explanations">Evaluation of Explanations&lt;/h3>
&lt;p>The quality of an explanation is assessed using fidelity metrics. For each explanation subgraph, the underlying cadence prediction model is run on the subgraph alone or on the input graph without the explanation subgraph. If the correct cadence label can be predicted using only the explanation subgraph, the explanation is deemed sufficient. Conversely, if the label changes when the input graph is used without the explanation subgraph, the explanation is considered necessary. An explanation that is both necessary and sufficient achieves a perfect fidelity score, indicating it provides a comprehensive and accurate insight into the model’s decision-making process.&lt;/p>
&lt;h2 id="interactive-visualization">Interactive Visualization&lt;/h2>
&lt;p>The framework features an interactive web interface built with the Verovio music engraving library. Users can click on individual notes to see the subgraphs that most contribute to the model’s predictions and the feature importance of each note. This interface not only makes the explanations accessible but also aligns them with traditional music notation, making it easier for musicians and researchers to understand.&lt;/p>
&lt;h2 id="real-world-applications-mozart-to-chopin">Real-World Applications: Mozart to Chopin&lt;/h2>
&lt;h3 id="mozarts-piano-sonata-k280">Mozart’s Piano Sonata K280&lt;/h3>
&lt;p>In an excerpt from Mozart’s Piano Sonata K280, the underlying GNN analysis model accurately identified a perfect authentic cadence. Using the SMUG-Explain framework we can see that the explanations outline the descending melodic line and the bass arpeggiation leading to the cadence, aligning closely with traditional harmonic analysis. This example highlights the framework’s potential to support musicological research.&lt;/p>
&lt;p>
&lt;figure id="figure-potential-reduction-process-of-smug-explain-pac-indicates-the-predicted-arrival-point-of-the-cadence">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/smug.gif" alt="Mozart&amp;amp;rsquo;s Piano Sonata K280" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Potential reduction process of Smug-Explain. PAC indicates the predicted arrival point of the cadence.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>When we isolate the explanation and apply a reduction process, retaining only the subgraph involved in the explanation, we are left with the essential structure of a textbook harmonic Perfect Authentic Cadence (PAC). This streamlined subgraph highlights the critical notes and relationships that define the PAC. While this process shows how SMUG-Explain can break down complex musical structures into their core elements, due to the complexity of musical reduction it isn’t a built-in feature just yet.&lt;/p>
&lt;h3 id="chopins-nocturne-in-c-minor">Chopin’s Nocturne in C Minor&lt;/h3>
&lt;p>In Chopin’s Nocturne, SMUG-Explain correctly identified a complex cadence despite unconventional voice leading. The explanation subgraph included notes from earlier measures, indicating the model’s consideration of long-range musical dependencies, much like human musicological analysis. Notice how the explanation highlights the first chord in the first measure probably as an indication of the key.&lt;/p>
&lt;p>
&lt;figure id="figure-explanation-of-the-cadence-detection-in-chopins-nocturne-in-c-minor">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/choping_smug.webp" alt="Chopin&amp;amp;rsquo;s Nocturne in C Minor" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Explanation of the cadence detection in Chopin&amp;rsquo;s Nocturne in C Minor.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h2 id="future-directions">Future Directions&lt;/h2>
&lt;p>SMUG-Explain is paving the way for making GNNs in music more interpretable and user-friendly. Future work will focus on developing new explanation techniques tailored to musical data, enhancing user-based evaluations, and making the framework more accessible through online platforms.
Limitations of SMUG-Explain&lt;/p>
&lt;p>While SMUG-Explain represents a significant step forward in making Graph Neural Networks (GNNs) more interpretable in the context of musical scores, it is not without its limitations. Understanding these constraints is crucial for setting realistic expectations and guiding future improvements. Here are some key limitations of the SMUG-Explain framework:&lt;/p>
&lt;h4 id="1-model-dependence">1. Model Dependence&lt;/h4>
&lt;p>An important limitation of SMUG-Explain lies in its dependence on the underlying GNN model used for cadence detection. Since SMUG-Explain employs post-hoc explanation methods, the accuracy and quality of its explanations are directly tied to the performance of the cadence detection model.&lt;/p>
&lt;p>The explanations generated by SMUG-Explain are only as good as the predictions made by the cadence detection model. If the underlying model has poor accuracy or fails to capture essential musical elements, the explanations provided will reflect these shortcomings. This means that any inaccuracies or biases in the model will be present in the explanations, potentially leading to misleading insights.&lt;/p>
&lt;p>The performance of the cadence detection model depends heavily on the quality and comprehensiveness of the training data. If the training dataset lacks diversity or contains errors, the model’s ability to generalize to new pieces will be compromised. Consequently, the explanations generated for these predictions may not be reliable or musically meaningful.&lt;/p>
&lt;p>Some musical compositions, especially those with complex structures and unconventional harmonies, might pose significant challenges for the cadence detection model. If the model struggles with these complexities, the explanations provided by SMUG-Explain may fail to accurately represent the underlying musical relationships, leading to incomplete or incorrect interpretations.&lt;/p>
&lt;h4 id="2-complexity-of-graph-constructions">2. Complexity of Graph Constructions&lt;/h4>
&lt;p>Creating accurate graph representations of musical scores is inherently complex. While SMUG-Explain uses onset, consecutive, during, and rest edges to model relationships between notes, this approach might not capture all the nuances of a musical piece. Elements like dynamics, articulations, and tempo changes are not explicitly represented in the current graph model, potentially overlooking important musical information.&lt;/p>
&lt;h4 id="3-user-experience-and-accessibility">3. User Experience and Accessibility&lt;/h4>
&lt;p>Although the interactive interface of SMUG-Explain makes it accessible to users, it still requires a certain level of technical proficiency to operate effectively. Users need to be familiar with both music theory and the technical aspects of GNNs to fully leverage the framework. Additionally, the current implementation relies on local deployment, which might be a barrier for widespread use. An online, server-based version could enhance accessibility but has yet to be developed.&lt;/p>
&lt;h4 id="4-interpretation-of-explanations">4. Interpretation of Explanations&lt;/h4>
&lt;p>While SMUG-Explain provides visual explanations that align with traditional music notation, interpreting these explanations still requires a deep understanding of music theory and machine learning. The explanations might not be immediately intuitive to all users, particularly those without a background in musicology or computational music research.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>SMUG-Explain bridges the gap between the sophisticated world of GNNs and the nuanced demands of musicology. By providing clear, interactive explanations directly within the context of musical scores, it empowers users to understand and trust the decisions of AI models, paving the way for more effective and insightful musical analyses.&lt;/p>
&lt;p>If you’re intrigued and want to explore SMUG-Explain further, check out the code on &lt;a href="https://github.com/manoskary/SMUG-Explain" target="_blank" rel="noopener">GitHub&lt;/a> or &lt;a href="https://arxiv.org/abs/2405.09241" target="_blank" rel="noopener">read the paper&lt;/a>. Dive in and discover how AI can illuminate the intricate beauty of musical compositions.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>I extend my gratitude to Francesco Foscarin, co-author of the original paper, for his invaluable assistance with the writing of this blog post and for providing some of the graphics.&lt;/p></description></item><item><title>Perception-Inspired Graph Convolution for Music Understanding Tasks</title><link>https://emmanouil-karystinaios.github.io/publication/2024_ijcai_musgconv/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2024_ijcai_musgconv/</guid><description/></item><item><title>SMUG-Explain: A Framework for Symbolic Music Graph Explanations</title><link>https://emmanouil-karystinaios.github.io/publication/2024_smc_smugexplain/</link><pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2024_smc_smugexplain/</guid><description/></item><item><title>8+8=4: Formalizing Time Units to Handle Symbolic Music Durations</title><link>https://emmanouil-karystinaios.github.io/publication/2023_cmmr_durationalgebra/</link><pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2023_cmmr_durationalgebra/</guid><description/></item><item><title>Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features</title><link>https://emmanouil-karystinaios.github.io/publication/2023_ismir_chordgnn/</link><pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2023_ismir_chordgnn/</guid><description/></item><item><title>Symbolic Music Representations for Classification Tasks: A Systematic Evaluation</title><link>https://emmanouil-karystinaios.github.io/publication/2023_ismir_symrep/</link><pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2023_ismir_symrep/</guid><description/></item><item><title>Automatic Note-Level Score-to-Performance Alignments in the ASAP Dataset</title><link>https://emmanouil-karystinaios.github.io/publication/2023_tismir_asap/</link><pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2023_tismir_asap/</guid><description/></item><item><title>Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem</title><link>https://emmanouil-karystinaios.github.io/publication/2023_ijcai_vocsep/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2023_ijcai_vocsep/</guid><description/></item><item><title>The ACCompanion: Combining Reactivity, Robustness, and Musical Expressivity in an Automatic Piano Accompanist</title><link>https://emmanouil-karystinaios.github.io/publication/2023_ijcai_accompanion/</link><pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2023_ijcai_accompanion/</guid><description/></item><item><title>Cadence Detection in Symbolic Classical Music using Graph Neural Networks</title><link>https://emmanouil-karystinaios.github.io/publication/2022_ismir_cadet/</link><pubDate>Tue, 30 Aug 2022 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2022_ismir_cadet/</guid><description/></item><item><title>partitura: A Python Package for Handling Symbolic Musical Data</title><link>https://emmanouil-karystinaios.github.io/publication/2022_mec_partitura/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2022_mec_partitura/</guid><description/></item><item><title>The match file format: Encoding Alignments between Scores and Performances</title><link>https://emmanouil-karystinaios.github.io/publication/2022_mec_match/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2022_mec_match/</guid><description/></item><item><title>Towards Quantifying Differences in Expressive Piano Performances: Are Euclidean-like Distance Measures Enough?</title><link>https://emmanouil-karystinaios.github.io/publication/2021_rppw_expressive/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2021_rppw_expressive/</guid><description/></item><item><title>Slides</title><link>https://emmanouil-karystinaios.github.io/slides/example/</link><pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/slides/example/</guid><description>&lt;h2 id="on-performance-similarity-and-structure-segmentation">On Performance Similarity and Structure Segmentation&lt;/h2>
&lt;p>&lt;a href="emmanouil-karystinaios.netify.app">Emmanouil Karystinaios&lt;/a> | &lt;a href="emmanouil-karystinaios.netify.app">article&lt;/a>&lt;/p>
&lt;!-- [Wowchemy](https://wowchemy.com/) | [Documentation](https://owchemy.com/docs/managing-content/#create-slides) -->
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Our objective is to find a way to quantify similarity of performances for which we have a matched score.
But first, we have to answer the following questions :&lt;/p>
&lt;ul>
&lt;li>What is performance similarity&lt;/li>
&lt;li>How to measure siilarity in match scores?&lt;/li>
&lt;li>Which similarity metric to use ?&lt;/li>
&lt;li>Why measure performance similarity?&lt;/li>
&lt;li>What elements affect our perception of performance similarity?&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="self-similarity-matrices">Self-Similarity Matrices&lt;/h2>
&lt;p>The concept of self-similarity matrices is fundamental for
capturing structural properties of music recordings. Generally,
one starts with a feature space L containing the elements of
the feature sequence under consideration as well as with a
similarity measure $f : \mathcal{L} \times \mathcal{L} \to \mathbb{R}$.&lt;/p>
&lt;hr>
&lt;h2 id="controls">Controls&lt;/h2>
&lt;ul>
&lt;li>Next: &lt;code>Right Arrow&lt;/code> or &lt;code>Space&lt;/code>&lt;/li>
&lt;li>Previous: &lt;code>Left Arrow&lt;/code>&lt;/li>
&lt;li>Start: &lt;code>Home&lt;/code>&lt;/li>
&lt;li>Finish: &lt;code>End&lt;/code>&lt;/li>
&lt;li>Overview: &lt;code>Esc&lt;/code>&lt;/li>
&lt;li>Speaker notes: &lt;code>S&lt;/code>&lt;/li>
&lt;li>Fullscreen: &lt;code>F&lt;/code>&lt;/li>
&lt;li>Zoom: &lt;code>Alt + Click&lt;/code>&lt;/li>
&lt;li>&lt;a href="https://github.com/hakimel/reveal.js#pdf-export" target="_blank" rel="noopener">PDF Export&lt;/a>: &lt;code>E&lt;/code>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="interval-vectors">Interval Vectors&lt;/h2>
&lt;dl>
&lt;dt>&lt;strong>&lt;em>Definition&lt;/em>&lt;/strong>&lt;/dt>
&lt;dd>An interval vector is an array of natural numbers which summarize the intervals present in a set of pitch classes. There is an equivalence between complementary intervals within the octave for example $2^{nd}$ minor and $7^{nth}$ major intervals, etc.&lt;/dd>
&lt;/dl>
&lt;p>Examples :&lt;/p>
&lt;ul>
&lt;li>Any Major chord : $ [0, 0, 1, 1, 1, 0 ] $&lt;/li>
&lt;li>Any Minor chord : $ [0, 0, 1, 1, 1, 0 ] $&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="interval-vectors-and-cadences">Interval Vectors and Cadences&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>Typical Cadences&lt;/strong>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\textrm{V}$ &amp;amp; - &amp;amp; $\textrm{I}$&lt;/td>
&lt;td>authentic&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\textrm{I}$ &amp;amp; - &amp;amp; $\textrm{V}$&lt;/td>
&lt;td>half&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\textrm{IV}$ &amp;amp; - &amp;amp; $\textrm{I}$&lt;/td>
&lt;td>plagal&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\textrm{V}$ &amp;amp; - &amp;amp; $\textrm{VI}$&lt;/td>
&lt;td>deceptive&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Interval Vectors are commutative, i.e. $\textrm{V} \to \textrm{I} = \textrm{I} \to \textrm{V} $:&lt;/p>
&lt;ul>
&lt;li>$ (\textrm{V/I})_{maj} = [1, 2, 2, 2, 3, 0] $&lt;/li>
&lt;li>$ (\textrm{V/I})_{min} = [2, 1, 2, 3, 2, 0] $&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="code-highlighting">Code Highlighting&lt;/h2>
&lt;p>Inline code: &lt;code>variable&lt;/code>&lt;/p>
&lt;p>Code block:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">porridge&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;blueberry&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="n">porridge&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;blueberry&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Eating...&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;p>For every window $w_i^j$ we compute the interval vector of all notes in thye window/ Therefore, $\textrm{Int_Vec}(w_i^j)\in \mathbb{N}^6$.
$$
\mathcal(W) = \sum_{i=1}^N\sum_{j=1}^\nu \textrm{Int_Vec}(w_i^j)
$$
Then, let $\mathcal{X}$ be some matrix decomposition of $\mathcal{W}$. The SSM $\mathcal{S}$ of $\mathcal{X}$ is:
$$
\mathcal{S} = \frac{\mathcal{X} \cdot \mathcal{X}}{| \mathcal{X} |^2}
$$&lt;/p>
&lt;hr>
&lt;h2 id="test">Test&lt;/h2>
&lt;p>\begin{tikzpicture}
\matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
{
F &amp;amp; B \
&amp;amp; A \};
\path[-stealth]
(m-1-1) edge node [above] {$\beta$} (m-1-2)
(m-1-2) edge node [right] {$\rho$} (m-2-2)
(m-1-1) edge node [left] {$\alpha$} (m-2-2);
\end{tikzpicture}&lt;/p>
&lt;hr>
&lt;h2 id="fragments">Fragments&lt;/h2>
&lt;p>Make content appear incrementally&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">{{% fragment %}} One {{% /fragment %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{% fragment %}} Three {{% /fragment %}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Press &lt;code>Space&lt;/code> to play!&lt;/p>
&lt;span class="fragment " >
One
&lt;/span>
&lt;span class="fragment " >
&lt;strong>Two&lt;/strong>
&lt;/span>
&lt;span class="fragment " >
Three
&lt;/span>
&lt;hr>
&lt;p>A fragment can accept two optional parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>class&lt;/code>: use a custom style (requires definition in custom CSS)&lt;/li>
&lt;li>&lt;code>weight&lt;/code>: sets the order in which a fragment appears&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="speaker-notes">Speaker Notes&lt;/h2>
&lt;p>Add speaker notes to your presentation&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="line">&lt;span class="cl">{{% speaker_note %}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">-&lt;/span> Only the speaker can read these notes
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">-&lt;/span> Press &lt;span class="sb">`S`&lt;/span> key to view
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{% /speaker_note %}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Press the &lt;code>S&lt;/code> key to view the speaker notes!&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Only the speaker can read these notes&lt;/li>
&lt;li>Press &lt;code>S&lt;/code> key to view&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;hr>
&lt;h2 id="themes">Themes&lt;/h2>
&lt;ul>
&lt;li>black: Black background, white text, blue links (default)&lt;/li>
&lt;li>white: White background, black text, blue links&lt;/li>
&lt;li>league: Gray background, white text, blue links&lt;/li>
&lt;li>beige: Beige background, dark text, brown links&lt;/li>
&lt;li>sky: Blue background, thin dark text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;ul>
&lt;li>night: Black background, thick white text, orange links&lt;/li>
&lt;li>serif: Cappuccino background, gray text, brown links&lt;/li>
&lt;li>simple: White background, black text, blue links&lt;/li>
&lt;li>solarized: Cream-colored background, dark green text, blue links&lt;/li>
&lt;/ul>
&lt;hr>
&lt;section data-noprocess data-shortcode-slide
data-background-image="blue_background.jpg"
>
&lt;h2 id="custom-slide">Custom Slide&lt;/h2>
&lt;p>Customize the slide style and background&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-markdown" data-lang="markdown">&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">background-image&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;/media/boards.jpg&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">background-color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;#0000FF&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">{{&lt;span class="p">&amp;lt;&lt;/span> &lt;span class="nt">slide&lt;/span> &lt;span class="na">class&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;my-style&amp;#34;&lt;/span> &lt;span class="p">&amp;gt;&lt;/span>}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h2 id="custom-css-example">Custom CSS Example&lt;/h2>
&lt;p>Let&amp;rsquo;s make headers navy colored.&lt;/p>
&lt;p>Create &lt;code>assets/css/reveal_custom.css&lt;/code> with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-css" data-lang="css">&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h1&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h2&lt;/span>&lt;span class="o">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">reveal&lt;/span> &lt;span class="nt">section&lt;/span> &lt;span class="nt">h3&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">color&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="kc">navy&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h1 id="questions">Questions?&lt;/h1>
&lt;p>&lt;a href="https://github.com/wowchemy/wowchemy-hugo-modules/discussions" target="_blank" rel="noopener">Ask&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">Documentation&lt;/a>&lt;/p></description></item><item><title>Harmonic Trajectories in the Tonnetz</title><link>https://emmanouil-karystinaios.github.io/post/tonnetz/</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/post/tonnetz/</guid><description>&lt;h1 id="harmonic-trajectories-in-the-tonnetz">Harmonic Trajectories in the Tonnetz&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The Neo-Riemannian Tonnetz is a way of visualizing musical relationships between chords. It was developed by music theorists to help understand how chords can transition smoothly from one to another.&lt;/p>
&lt;p>Imagine a grid or network of interconnected points. Each point represents a different chord. The horizontal lines connect chords that are closely related, while the vertical lines connect chords that share similar tonal qualities.&lt;/p>
&lt;p>The Tonnetz is based on the idea that chords can be transformed or changed into one another through small movements. These transformations are represented by diagonal lines on the Tonnetz. For example, a chord can be transformed into another chord by changing one note at a time, moving in a specific direction on the grid.&lt;/p>
&lt;p>By studying the Tonnetz, musicians and theorists can analyze chord progressions and see how different chords are related to each other. It provides a visual representation of the harmonic possibilities and helps to explain the underlying structure of music.&lt;/p>
&lt;p>In simple terms, the Neo-Riemannian Tonnetz is a grid that shows how chords in music are connected and can be transformed smoothly from one to another. It helps musicians and theorists understand how chords fit together and how they can create pleasing transitions in music.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>PLR operations, also known as Parallel, Leading Tone, and Relative operations, are a set of chord transformations used in music theory. These operations help musicians understand how chords can be changed while maintaining a similar musical function.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Parallel operation: This operation involves changing a chord to another chord with the same root note but a different quality. For example, if you have a major chord, you can transform it into a minor chord by lowering the third note. This maintains the same root note but gives a different emotional quality to the chord.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Leading Tone operation: The leading tone refers to the seventh note of a major scale, which is one step below the tonic note. In this operation, a chord is transformed by changing the root note to its leading tone. This creates a sense of tension and a desire to resolve back to the tonic chord. For example, if you have a C major chord, you can transform it into a B diminished chord by changing the root note to B.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Relative operation: This operation involves changing a chord to another chord that shares a similar tonal quality but has a different root note. For example, if you have a C major chord, you can transform it into an A minor chord by changing the root note to A. The relative operation allows for smooth transitions between chords that have a related sound.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>These PLR operations provide musicians with tools to transform chords while maintaining certain musical characteristics. By understanding these operations, musicians can create interesting chord progressions, establish tension and resolution, and explore different harmonic possibilities in their music.&lt;/p>
&lt;p>The &lt;strong>PLR operations&lt;/strong> are the foundation of the Tonnetz.&lt;/p>
&lt;p>These operations/ transformations are the principal transformations of the Neo-Riemmanian theory which was mainly conceived by David Lewin (1933–2003).
The mirror process is relative to the chord intervalic relations not the position of the shape in the circle.&lt;/p>
&lt;h4 id="plr-as-group-operations">PLR as Group Operations&lt;/h4>
&lt;p>The PLR group acts simply transitively on the set {$ n_M,n=0…11$} $\cup$ { $n_m,n=0…11 $} of the 24 major and minor triads, where $n_M$ (resp. $n_m$) represents a major (resp. minor) triad with root n in the usual semi-tone encoding of pitch classes.&lt;/p>
&lt;p>It is isomorphic to the dihedral group D24 of order 24, and is generated by the following two transformations.&lt;/p>
&lt;p>The transformation $L: \mathbb{Z} _{24} \to \mathbb{Z} _{24}$ is called the leading-tone operation, and is such that:&lt;/p>
&lt;p>$$
L(n_M)=(n+4)_m \textrm{ and the complementary } L(n_m)=(n+8)_M &lt;br>
$$&lt;/p>
&lt;p>The transformation $R: \mathbb{Z} _{24} \to \mathbb{Z} _{24}$ is called the relative operation, and is such that:&lt;/p>
&lt;p>$$
R(n_M)=(n+9)_m \textrm{ and similarly the complementary }
$$&lt;/p>
&lt;p>Though not a generator, the operation $P=(RL)3R$, called the parallel operation, is often considered, and is such that $P(n_M)=n_m$&lt;/p>
&lt;h2 id="trajectories-in-the-tonnetz">Trajectories in the Tonnetz&lt;/h2>
&lt;p>The trajectory is defined as a path $\mathcal{X} $ in the Tonnetz $T$, i.e. an ordered list of positions in the space $T$.&lt;/p>
&lt;p>Let us investigate some basic scenarios for trajectory construction.
Placing the first note in the Tonnetz has no bearing on the descriptors we ultimately compute,
so we can simply pick an arbitrary position. Now we consider the case where we have to place two notes:
one of them is placed as in the previous case, and the second one is placed according to a criterion
depending on a distance measure. To this end, we define a function $dist: \mathbb{Z} _{12} \times \mathbb{Z} _{12} \to \mathbb{N}$,
which assigns to the pitch class representation of notes, $x$ and $y$, their distance according to a given Tonnetz as:
\begin{equation}
dist(x, y) =
\begin{cases}
0 &amp;amp; \text{if } x=y \
1 &amp;amp; \text{if } (x-y)\in T \lor (y-x)\in T \
2 &amp;amp; \text{otherwise} &lt;br>
\end{cases}
\end{equation}
Note that, $ dist(x, y) = dist(y, x) $.
By abuse of notation, from now on when referring to notes or chords we automatically consider the numerical representations of their pitch class (PC). They are defined with integer notation, where $C=0$, $C#=1$, $D=2$, etc. Accordingly, chords are PC sets.&lt;/p>
&lt;p>
&lt;figure id="figure-figure-1--the-representation-of-a-c-major-chord-ie-thepitch-class-set-cmaj--0-4-7-in-four-different-ton-netze-the-notes-of-the-chord-are-illustrated-in-blue-theintermediate-edges-and-notes-connecting-the-chord-repre-sentation-are-denoted-in-red-the-note-c--0-is-alwaysplaced-at-point-0-0">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/Cmaj_Tonnetze.png" alt="C major Tonnetze." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Figure 1 : The representation of a C major chord, i.e. the
pitch class set Cmaj = {0, 4, 7}, in four different Ton-
netze. The notes of the chord are illustrated in blue. The
intermediate edges and notes (connecting the chord repre-
sentation) are denoted in red. The note C = 0 is always
placed at point (0, 0).
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Two notes $x, y$ are neighbors if $dist(x, y)=1$. Thus, in the case where two notes are neighbors we find which kind of interval they form and to which Tonnetz axis this interval corresponds. In the case where $dist(x, y)=2$, we define a positioning according to a shared neighbor.
For example, in Tonnetz $T( 1, 2, 9)$, the placement of note E in relation to note C is computed using the shared neighbor D: D is first placed in relation to C, then E is placed in relation to D. This example is illustrated in Figure 1 (horizontal axis). The intermediate neighbors are denoted in red.&lt;/p>
&lt;p>Given a note $x$ and a position $p$ and a fixed Tonnetz $T$, let $\pi(x, p)$ be a positioning function for $T$ which, from the reference position $p$, places the note $x$ as described above.&lt;/p>
&lt;p>We now move on to chords, which we will demonstrate on the simple case of a triad but generalize as well to chords of any size. In Figure 1 a C major chord is represented in 4 different Tonnetze, $T(1, 3, 8)$, $T(1, 2, 9)$, $T(3, 4, 5)$ and $T(2, 3, 7)$. From this representation we can see that in $T(3, 4, 5)$ the chord forms a connected graph while in the other cases the graph is disconnected. In all representations, we place the note $C= 0$ %\Isa{la notation PC n&amp;rsquo;est pas introduite}
at point $(0, 0)$. From there, we place the other notes based on the Tonnetz intervals and periodicity. For example, we first need to find if E and G are neighbors of C in Tonnetz $T$.
For this we consider the following function, which gives the neighbors in the chord $X$ of note $y$ according to Tonnetz $T$:
\begin{equation}
neigh(y, X, T) =
{ x\in X \mid dist(y, x)=1 \text{ in } T} %, &amp;amp; \text{otherwise}
\end{equation}
In the C major scenario, E and G are neighbors of C in Tonnetz $T(3, 4, 5)$ so we can easily find their place in $T$. We define a function which takes a chord $X$ and a position $p$ in a fixed space $T$ and assigns positions to all notes of $X$ as follows:
\begin{equation}
f(X, p) = { \pi(x, p) \mid x \in X }
\end{equation}&lt;/p>
&lt;p>If a chord does not strictly consist of neighboring notes, we first place notes which are neighbors of $y$, then we attempt to place the remaining notes according to the newly positioned notes, repeating until no more notes can be placed.
For a note $y$ with corresponding position $p_y$, a chord $X$ and a Tonnetz $T$, first finds the neighbors of $y$ and then the neighbors of each neighbor of $y$ in $T$, etc.
If some notes remain to be placed, then one of the remaining notes is placed in relation to an arbitrary already placed note, and the process is repeated.&lt;/p>
&lt;h2 id="chord-trajectory-construction">Chord Trajectory Construction&lt;/h2>
&lt;p>This process, detailed in Algorithm 1, ensures a methodical approach to positioning each note within a chord on the Tonnetz grid. By iteratively placing neighboring notes, the algorithm builds a coherent and connected representation of the chord. If any notes remain unplaced after the initial neighbor placements, the algorithm continues by arbitrarily positioning the remaining notes relative to those already placed. This iterative process guarantees that all notes find their appropriate positions on the Tonnetz.&lt;/p>
&lt;h3 id="algorithm-1-placement-of-the-first-chord">Algorithm 1: Placement of the First Chord&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Input&lt;/strong>: A chord ( X ) and a Tonnetz ( T )&lt;/li>
&lt;li>&lt;strong>Result&lt;/strong>: A set of placed notes&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Steps&lt;/strong>:&lt;/p>
&lt;ol>
&lt;li>Choose an arbitrary note ( x ) from ( X ) and assign it a position ( p_x ).&lt;/li>
&lt;li>Initialize &lt;code>placed&lt;/code> with ( {(x, p_x)} ).&lt;/li>
&lt;li>Initialize &lt;code>to_place&lt;/code> with ( X \setminus {x} ).&lt;/li>
&lt;li>While &lt;code>to_place&lt;/code> is not empty:
&lt;ul>
&lt;li>While there exists a note ( x ) in &lt;code>placed&lt;/code> and a neighbor ( y ) in &lt;code>to_place&lt;/code>:
&lt;ul>
&lt;li>Place ( y ) in relation to ( x ) and update &lt;code>placed&lt;/code> and &lt;code>to_place&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If no neighbors are found, place a remaining note ( y ) relative to an arbitrary note ( x ) in &lt;code>placed&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>This algorithm ensures that the positioning of each chord is systematic and consistent, leveraging the geometric structure of the Tonnetz.&lt;/p>
&lt;h3 id="extending-to-chord-sequences">Extending to Chord Sequences&lt;/h3>
&lt;p>For sequences of chords, the placement must consider the relationships between consecutive chords. The trajectory ( \mathcal{X} ) for a sequence of chords ( [C_1, C_2, \ldots, C_k] ) is built by extending the positioning strategy to maintain coherence across the entire sequence.&lt;/p>
&lt;p>Given a chord ( C_n ) with known positions for ( C_{n-1} ), the positioning for ( C_n ) is determined by selecting optimal reference points from ( C_{n-1} ) to ensure a compact representation. This process is repeated for each chord in the sequence.&lt;/p>
&lt;h3 id="example-i-iv-v-i-progression-in-c-major">Example: I-IV-V-I Progression in C Major&lt;/h3>
&lt;p>Consider the sequence I-IV-V-I in C major:&lt;/p>
&lt;ul>
&lt;li>Chords: C major, F major, G major, C major&lt;/li>
&lt;li>Pitch Class Sets: ({0, 4, 7}, {5, 9, 0}, {7, 11, 2}, {0, 4, 7})&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>&lt;strong>C Major&lt;/strong>: Position C at (0,0) and place E and G accordingly.&lt;/li>
&lt;li>&lt;strong>F Major&lt;/strong>: Position F major relative to C, using common note C as a reference.&lt;/li>
&lt;li>&lt;strong>G Major&lt;/strong>: Position G major relative to both F major and the subsequent C major to find the most compact placement.&lt;/li>
&lt;li>&lt;strong>C Major&lt;/strong>: Reposition C major with reference to G major.&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure id="figure-figure-2--this-example-demonstrates-how-the-trajectory-method-ensures-a-compact-and-connected-representation-of-chord-sequences-leveraging-shared-notes-and-optimal-positioning-strategies-independently-of-the-chosen-tonnetz-space">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./figs/tonnetz_i-iv-v-i.png" alt="Progression in 4 Tonnetze." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Figure 2 :
This example demonstrates how the trajectory method ensures a compact and connected representation of chord sequences, leveraging shared notes and optimal positioning strategies, independently of the chosen Tonnetz Space.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="compliance-function-and-compactness">Compliance Function and Compactness&lt;/h3>
&lt;p>To determine the best Tonnetz for a given piece, the compliance function evaluates the compactness of trajectories across different Tonnetz configurations. The compliance predicate ensures that each chord is represented as a connected graph within the Tonnetz. The trajectory with the least number of connected components, smallest maximum width, and height is selected as the most compact.&lt;/p>
&lt;h4 id="compliance-function-steps">Compliance Function Steps:&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>Predicate Check&lt;/strong>: Ensure each chord forms a connected graph.&lt;/li>
&lt;li>&lt;strong>Trajectory Calculation&lt;/strong>: Build trajectories in all candidate Tonnetz.&lt;/li>
&lt;li>&lt;strong>Compactness Evaluation&lt;/strong>: Measure the width, height, and number of connected components of each trajectory.&lt;/li>
&lt;li>&lt;strong>Normalization and Selection&lt;/strong>: Normalize values and select the Tonnetz with the highest compactness score.&lt;/li>
&lt;/ol>
&lt;h3 id="graph-representation-centrality-measures-and-future-possibilities">Graph Representation Centrality Measures and Future Possibilities&lt;/h3>
&lt;p>Once a trajectory is constructed, it is transformed into a weighted graph. Each chord becomes a vertex, and edges represent the connections based on the Tonnetz intervals. Centrality measures (Katz, harmonic, closeness) are then calculated to capture the structural properties of the graph.
These centrality measures, along with the general MIDI descriptors, form the feature set for classification algorithms, such as Random Forest and k-Nearest Neighbors.&lt;/p>
&lt;p>Furthermore, the graph representation of harmonic trajectories in the Tonnetz opens a gateway to advanced deep learning methods, particularly leveraging graph neural networks (GNNs). By transforming musical sequences into weighted, non-directed graphs, where vetrices model notes and edges their configuration in the Tonnetz space, this method enables the use of GNNs to learn intricate relationships within the music data. GNNs can effectively model the dependencies and patterns in these graph structures, allowing for potential improvements on classification tasks. This deep learning approach can uncover latent harmonic features and complex interactions that traditional methods might miss, providing a more robust framework for tasks such as composer classification, tonal tension, and even compositional style analysis.&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>The Tonnetz-based trajectory method offers a powerful tool for harmonic analysis and genre classification in music. By representing chords and their transitions in a geometric space, this method provides a nuanced understanding of harmonic relationships. The compliance function and centrality measures ensure that the trajectories are both compact and informative, leading to high classification accuracy.&lt;/p>
&lt;p>Future work will explore applying geometrical deep learning techniques, such as graph convolutional networks (GCNs) and graph attention networks (GATs), to further enhance the analysis and classification of harmonic trajectories. By leveraging these advanced models, we aim to capture the complex, non-Euclidean relationships inherent in musical compositions, enabling more precise and insightful characterizations of musical styles and genres. This approach may provide deeper insights into the underlying harmonic structures and stylistic nuances of different musical pieces.&lt;/p></description></item><item><title>Music genre descriptor for classification based on Tonnetz trajectories</title><link>https://emmanouil-karystinaios.github.io/publication/2020_jim_tonnetz/</link><pubDate>Mon, 30 Nov 2020 00:00:00 +0000</pubDate><guid>https://emmanouil-karystinaios.github.io/publication/2020_jim_tonnetz/</guid><description/></item></channel></rss>