[{"authors":null,"categories":null,"content":"Emmanouil Karystinaios is a researcher in artificial intelligence, recently completing his Ph.D. at the Computational Perception Institute of Johannes Kepler University. His research focuses on Graph Neural Networks, Computational Musicology, and Music Information Retrieval. Currently, he is working on the Automatic Analysis of Symbolic Music using Graph Neural Networks (GNNs) and on Generative Music Medicine, exploring the use of AI-generated music in therapeutic contexts.\nHis past and ongoing work includes Cadence Detection, Structural Segmentation, and developing Python packages such as Partitura and GraphMuse for symbolic music processing.\n  Download my resumé.\n","date":1731283200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1731283200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Emmanouil Karystinaios is a researcher in artificial intelligence, recently completing his Ph.D. at the Computational Perception Institute of Johannes Kepler University. His research focuses on Graph Neural Networks, Computational Musicology, and Music Information Retrieval.","tags":null,"title":"Emmanouil Karystinaios","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://emmanouil-karystinaios.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Emmanouil Karystinaios"],"categories":["Large Language Models","Music Medicine","Generation"],"content":"Recent advances in generative AI has reached the music field. Many generative music models can lead to awesome creations. But there is one compelling new application that has been left until now unexplored, music therapy. This research, originally presented at the ISMIR 2024 Conference as a late breaking demo, introduces a novel model for generating music for music therapy purposes, named Language Models for Music Medicine Generation. The paper explores how AI-driven music can support mental health by guiding listeners through emotional transitions using generative music designed to align with established therapeutic principles.\nBackground: The Therapeutic Power of Music Music therapy, recognized since the 19th century, has been scientifically shown to provide significant mental health benefits, such as reducing anxiety and improving mood. While traditional music therapy is conducted by certified professionals, Music Medicine offers a new approach: self-administered music interventions that can be beneficial between therapist-guided sessions.\nModel Overview: Fine-Tuning MusicGen for Therapy To create a therapeutic experience, the researchers fine-tuned MusicGen, a state-of-the-art generative model initially trained on vast amounts of instrumental music, using the MTG-Jamendo dataset annotated with emotion and mood labels. They adapted MusicGen to produce music that aligns with specific emotional states, using a method known as low-rank adaptation (LoRA), which enables efficient fine-tuning without altering the original model’s core weights.\nThe Iso Principle and Emotional Transitions Central to this model is the iso principle, a technique in music therapy where music reflecting the listener’s current emotional state gradually shifts to evoke a more desired emotional outcome. The model generates sequences of 30-second audio clips, each crafted to represent an intermediate emotional state between the listener’s current mood and their targeted emotional state. These clips are then combined to form a cohesive 15-minute “music medicine” session that smoothly transitions across a spectrum of emotions.\nMethodology: Creating Emotional Continuity Each audio clip is generated through prompt engineering, where tags define the emotional state, preferred instrument, and genre. To create a seamless experience, the clips are normalized, trimmed, and then overlapped and crossfaded. The model’s parameters, such as genre and instrumentation, are adjusted to mirror the emotion distribution in the dataset, adding depth to the experience.\nResults and Performance Evaluation The team tested three versions of the fine-tuned model—MusicGen-Small, Medium, and Large—each exhibiting varying improvements over the base model in capturing mood. The model’s output was evaluated using metrics such as the Contrastive Language-Audio Pretraining (CLAP) score, which measures audio relevance to the intended emotion. Additionally, precision-recall and Hamming scores validated that the music aligned with targeted emotional states.\nFuture Directions This research demonstrates a new frontier in AI-assisted mental health care, with the potential for Music Medicine to serve as a bridge between formal therapy sessions. The team plans to conduct user studies to assess the model’s effectiveness and work on more fluid transitions between emotional states for an even more natural experience. As AI continues to evolve, such innovations point to a future where technology enhances well-being through immersive potentially multi-sensor, emotionally resonant interventions.\nAknowledgements This work, titled Language Models for Music Medicine Generation, was collaboratively developed by Emmanouil Nikolakakis, Joann Ching, Emmanouil Karystinaios, Gabrielle Sipin, Gerhard Widmer, and Razvan Marinescu.\n","date":1731283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731283200,"objectID":"cc23ca7fe8b1974f8ec16d9f2294233f","permalink":"https://emmanouil-karystinaios.github.io/post/musicmedicine/","publishdate":"2024-11-11T00:00:00Z","relpermalink":"/post/musicmedicine/","section":"post","summary":"This research introduces a novel model for generating music for music therapy purposes, named *Language Models for Music Medicine Generation*. The paper explores how AI-driven music can support mental health by guiding listeners through emotional transitions using generative music designed to align with established therapeutic principles.","tags":["Academic"],"title":"Language Models for Music Medicine Generation","type":"post"},{"authors":["Emmanouil Karystinaios"],"categories":null,"content":"","date":1730419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730419200,"objectID":"06704028466a173cda3f427c7466b1c7","permalink":"https://emmanouil-karystinaios.github.io/publication/2024_thesis/","publishdate":"2024-11-01T00:00:00Z","relpermalink":"/publication/2024_thesis/","section":"publication","summary":"My PhD thesis resolves around my work on Symbolic Music analysis targeting Cadence Detection, Roman Numeral Analysis and Voice Separation.","tags":[],"title":"Symbolic Music Analysis with Graph Neural Networks","type":"publication"},{"authors":["Francesco Foscarin","Emmanouil Karystinaios","Eita Nakamura","Gerhard Widmer"],"categories":null,"content":"","date":1729382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729382400,"objectID":"400bf2e0802106e6eb6b2151f5493f38","permalink":"https://emmanouil-karystinaios.github.io/publication/2024_ismir_vocseppoly/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/publication/2024_ismir_vocseppoly/","section":"publication","summary":"In this work, we present a framework for separating the notes from a quantized symbolic music piece into multiple voices and staves. We propose an end-to-end system based on graph neural networks that clusters notes that belong to the same chord and connects them with edges if they are part of a voice.","tags":[],"title":"Cluster and Separate: a GNN Approach to Voice and Staff Prediction for Score Engraving","type":"publication"},{"authors":["Emmanouil Karystinaios","Gerhard Widmer"],"categories":null,"content":"","date":1729382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729382400,"objectID":"b833ae39db14c136cf853d0db23fd21f","permalink":"https://emmanouil-karystinaios.github.io/publication/2024_ismir_graphmuse/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/publication/2024_ismir_graphmuse/","section":"publication","summary":"GraphMuse is a graph processing framework and library that facilitates efficient music graph processing and GNN training for symbolic music tasks. Central to our contribution is a new neighbor sampling technique specifically targeted toward meaningful behavior in musical scores. Additionally, GraphMuse integrates hierarchical modeling elements that augment the expressivity and capabilities of graph networks for musical tasks.","tags":[],"title":"GraphMuse: A Library for Symbolic Music Graph Processing","type":"publication"},{"authors":["Emmanouil Karystinaios"],"categories":["Graph Neural Networks","Cadence","Interpretability"],"content":"Graph Neural Networks (GNNs) are making waves in the world of Music Information Research (MIR). From predicting cadences to generating expressive performances, these networks are unlocking new potentials in understanding and processing musical scores. But there’s a catch — their complex, “black-box” nature makes them hard to interpret. That’s where SMUG-Explain comes in. This innovative framework is designed to generate and visualize explanations for GNNs applied to musical scores. Let’s dive into how SMUG-Explain works, its application in cadence detection, and its potential to bring AI and musicology closer together.\nThe Challenge: Understanding GNNs in Music GNNs are fantastic at capturing intricate relationships in graph-structured data, which is perfect for tasks like cadence detection and voice separation in music. However, they’re notoriously difficult to interpret. For musicians and researchers, this opacity can be a big problem. Imagine having a super-smart assistant who helps you analyze music but never explains why it made certain decisions. Frustrating, right?\nEnter SMUG-Explain, or Score MUsic Graph Explain. This is a framework for interpreting GNNs applied to music but it also integrates these explanations directly into the musical scores. By visualizing how each note and its features contribute to the model’s predictions, SMUG-Explain makes GNNs more transparent and understandable.\nHow Does SMUG-Explain Work? Graph-Based Representation of Musical Scores SMUG-Explain transforms a musical score into a graph where notes are vertices, and edges represent their temporal relationships. It uses four types of edges:\n Onset edges: Connect notes that start together. Consecutive edges: Link notes where one ends as the next begins. During edges: Connect notes that occur within the duration of another note. Rest edges: Connect notes across rests.     In this example, we view the score graph on an excerpt of a Perfect Authentic Cadence (PAC) with harmonic annotations written in Roman numerals.  Explanation Techniques SMUG-Explain employs several post-hoc, gradient-based explanation methods such as Saliency, Integrated Gradients, Deconvolution, and Guided Backpropagation. These techniques evaluate the importance of each note and its features in predicting specific musical events, such as cadences. Additionally, these methods allow us to derive attribution weights for every edge in the input graph. By selecting the top-k most important edges, we can construct an explanation subgraph that highlights the critical components influencing the model’s predictions.\nEvaluation of Explanations The quality of an explanation is assessed using fidelity metrics. For each explanation subgraph, the underlying cadence prediction model is run on the subgraph alone or on the input graph without the explanation subgraph. If the correct cadence label can be predicted using only the explanation subgraph, the explanation is deemed sufficient. Conversely, if the label changes when the input graph is used without the explanation subgraph, the explanation is considered necessary. An explanation that is both necessary and sufficient achieves a perfect fidelity score, indicating it provides a comprehensive and accurate insight into the model’s decision-making process.\nInteractive Visualization The framework features an interactive web interface built with the Verovio music engraving library. Users can click on individual notes to see the subgraphs that most contribute to the model’s predictions and the feature importance of each note. This interface not only makes the explanations accessible but also aligns them with traditional music notation, making it easier for musicians and researchers to understand.\nReal-World Applications: Mozart to Chopin Mozart’s Piano Sonata K280 In an excerpt from Mozart’s Piano Sonata K280, the underlying GNN analysis model accurately identified a perfect authentic cadence. Using the SMUG-Explain framework we can see that the explanations outline the descending melodic line and the bass arpeggiation leading to the cadence, aligning closely with traditional harmonic analysis. This example highlights the framework’s potential to support musicological research.\n   Potential reduction process of Smug-Explain. PAC indicates the predicted arrival point of the cadence.  When we isolate the explanation and apply a reduction process, retaining only the subgraph involved in the explanation, we are left with the essential structure of a textbook harmonic Perfect Authentic Cadence (PAC). This streamlined subgraph highlights the critical notes and relationships that define the PAC. While this process shows how SMUG-Explain can break down complex musical structures into their core elements, due to the complexity of musical reduction it isn’t a built-in feature just yet.\nChopin’s Nocturne in C Minor In Chopin’s Nocturne, SMUG-Explain correctly identified a complex cadence despite unconventional voice leading. The …","date":1719446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719446400,"objectID":"1a56b53fc811de3277579778f9503ea5","permalink":"https://emmanouil-karystinaios.github.io/post/smug/","publishdate":"2024-06-27T00:00:00Z","relpermalink":"/post/smug/","section":"post","summary":"Visualizing Graph Neural Network Explanations for music.","tags":["Academic"],"title":"From Notes to Insights - Visualizing Graph Neural Network Explanations with SMUG-Explain","type":"post"},{"authors":["Emmanouil Karystinaios","Francesco Foscarin","Gerhard Widmer"],"categories":null,"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"bf39d83009dd050485a18ac98dd595d0","permalink":"https://emmanouil-karystinaios.github.io/publication/2024_ijcai_musgconv/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/publication/2024_ijcai_musgconv/","section":"publication","summary":"In this paper we propose a new graph convolutional block, called MusGConv, specifically designed for the efficient processing of musical score data and motivated by general perceptual principles. We evaluate our approach on four different musical understanding problems, such as monophonic voice separation, harmonic analysis, cadence detection, and composer identification which, in abstract terms, translate to different graph learning problems, namely, node classification, link prediction, and graph classification. Our experiments demonstrate that MusGConv improves the performance on three of the aforementioned tasks while being conceptually very simple and efficient.","tags":[],"title":"Perception-Inspired Graph Convolution for Music Understanding Tasks","type":"publication"},{"authors":["Emmanouil Karystinaios","Francesco Foscarin","Gerhard Widmer"],"categories":null,"content":"","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"0d4f67cae5172baf1ef3abd414550086","permalink":"https://emmanouil-karystinaios.github.io/publication/2024_smc_smugexplain/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/publication/2024_smc_smugexplain/","section":"publication","summary":"In this work, we present Score MUsic Graph (SMUG)-Explain, a framework for generating and visualizing explanations of graph neural networks applied to arbitrary prediction tasks on musical scores. Our system allows the user to visualize the contribution of input notes (and note features) to the network output, directly in the context of the musical score. We provide an interactive interface based on the music notation engraving library Verovio. We showcase the usage of SMUG-Explain on the task of cadence detection in classical music.","tags":[],"title":"SMUG-Explain: A Framework for Symbolic Music Graph Explanations","type":"publication"},{"authors":["Emmanouil Karystinaios","Francesco Foscarin","Florent Jacquemard","Masahiko Sakai","Satoshi Tojo","Gerhard Widmer"],"categories":null,"content":"","date":1693353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693353600,"objectID":"d3814f7af9b07d81069cf3c67acb64b5","permalink":"https://emmanouil-karystinaios.github.io/publication/2023_cmmr_durationalgebra/","publishdate":"2023-11-04T00:00:00Z","relpermalink":"/publication/2023_cmmr_durationalgebra/","section":"publication","summary":"In this paper we present a formalization of musical durations and their operations in a semiring framework.","tags":[],"title":"8+8=4: Formalizing Time Units to Handle Symbolic Music Durations","type":"publication"},{"authors":["Emmanouil Karystinaios","Gerhard Widmer"],"categories":null,"content":"","date":1693353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693353600,"objectID":"680c53227abfe9874b5de9728a266f79","permalink":"https://emmanouil-karystinaios.github.io/publication/2023_ismir_chordgnn/","publishdate":"2023-11-04T00:00:00Z","relpermalink":"/publication/2023_ismir_chordgnn/","section":"publication","summary":"In this paper we present a graph model for Roman Numeral analysis in symbolic music.","tags":[],"title":"Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features","type":"publication"},{"authors":["Huan Zhang","Emmanouil Karystinaios","Simon Dixon","Gerhard Widmer","Carlos Canchino Chacon"],"categories":null,"content":"","date":1693353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693353600,"objectID":"33ea8325a7096908c8121f1b18d371d2","permalink":"https://emmanouil-karystinaios.github.io/publication/2023_ismir_symrep/","publishdate":"2023-11-04T00:00:00Z","relpermalink":"/publication/2023_ismir_symrep/","section":"publication","summary":"In this paper, we present a series of systematic experiments to investigate the impact of symbolic representations for three piece-level tasks.","tags":[],"title":"Symbolic Music Representations for Classification Tasks: A Systematic Evaluation","type":"publication"},{"authors":["Silvan Peter","Carlos Cancino-Chacón","Francesco Foscarin","Andrew Philip McLeod","Florian Henkel","Emmanouil Karystinaios","Gerhard Widmer"],"categories":null,"content":"","date":1687737600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687737600,"objectID":"b1842d65d5bb6fab5ad2b985660e05e4","permalink":"https://emmanouil-karystinaios.github.io/publication/2023_tismir_asap/","publishdate":"2023-06-26T00:00:00Z","relpermalink":"/publication/2023_tismir_asap/","section":"publication","summary":"This paper introduces the (n)-ASAP dataset, a dataset with note level alignment annotations.","tags":[],"title":"Automatic Note-Level Score-to-Performance Alignments in the ASAP Dataset","type":"publication"},{"authors":["Emmanouil Karystinaios","Francesco Foscarin","Gerhard Widmer"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"6ea129a13f7288ba486fbad65a16f768","permalink":"https://emmanouil-karystinaios.github.io/publication/2023_ijcai_vocsep/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/2023_ijcai_vocsep/","section":"publication","summary":"In this paper we present a Heterogeneous Graph model of the musical score for Voice Separation in Symbolic Music.","tags":[],"title":"Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem","type":"publication"},{"authors":["Carlos Cancino-Chacón","Silvan Peter","Patricia Hu","Emmanouil Karystinaios","Florian Henkel","Francesco Foscarin","Nimrod Varga","Gerhard Widmer"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"ce65f755cd4f9393ef873df547397c18","permalink":"https://emmanouil-karystinaios.github.io/publication/2023_ijcai_accompanion/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/2023_ijcai_accompanion/","section":"publication","summary":"This paper introduces the ACCompanion, an expressive accompaniment system.","tags":[],"title":"The ACCompanion: Combining Reactivity, Robustness, and Musical Expressivity in an Automatic Piano Accompanist","type":"publication"},{"authors":["Emmanouil Karystinaios","Gerhard Widmer"],"categories":null,"content":"","date":1661817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661817600,"objectID":"dff6f8a3e34be5b041aa35f93523348b","permalink":"https://emmanouil-karystinaios.github.io/publication/2022_ismir_cadet/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/publication/2022_ismir_cadet/","section":"publication","summary":"In this paper we present a Graph model of musical score and a Graph Neural Network to detect cadences.","tags":[],"title":"Cadence Detection in Symbolic Classical Music using Graph Neural Networks","type":"publication"},{"authors":["Carlos Canchino Chacon","Silvan Peter","Emmanouil Karystinaios","Francesco Foscarin","Maarten Grachten","Gerhard Widmer"],"categories":null,"content":"","date":1653868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653868800,"objectID":"ce7d4993b6038b59908a9dba59102a68","permalink":"https://emmanouil-karystinaios.github.io/publication/2022_mec_partitura/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/publication/2022_mec_partitura/","section":"publication","summary":"In this paper we present partitura, a Python package music symbolic music processing.","tags":[],"title":"partitura: A Python Package for Handling Symbolic Musical Data","type":"publication"},{"authors":["Francesco Foscarin","Emmanouil Karystinaios","Silvan Peter","Carlos Canchino Chacon","Maarten Grachten","Gerhard Widmer"],"categories":null,"content":"","date":1653868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653868800,"objectID":"d160cc8c263200f6c3d8d35185729428","permalink":"https://emmanouil-karystinaios.github.io/publication/2022_mec_match/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/publication/2022_mec_match/","section":"publication","summary":"In this paper we present the match file format, a format for encoding alignments between performances and scores.","tags":[],"title":"The match file format: Encoding Alignments between Scores and Performances","type":"publication"},{"authors":["Carlos Canchino Chacon","Silvan Peter","Emmanouil Karystinaios","Gerhard Widmer"],"categories":null,"content":"","date":1648598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648598400,"objectID":"9b04d291ad1e8164872d64f51bb15214","permalink":"https://emmanouil-karystinaios.github.io/publication/2021_rppw_expressive/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/publication/2021_rppw_expressive/","section":"publication","summary":"In this work, we present some preliminary results about quantifying differences in expressive piano performances in the context of computational generative models.","tags":[],"title":"Towards Quantifying Differences in Expressive Piano Performances: Are Euclidean-like Distance Measures Enough?","type":"publication"},{"authors":["Emmanouil Karystinaios"],"categories":[],"content":"On Performance Similarity and Structure Segmentation Emmanouil Karystinaios | article\n Introduction Our objective is to find a way to quantify similarity of performances for which we have a matched score. But first, we have to answer the following questions :\n What is performance similarity How to measure siilarity in match scores? Which similarity metric to use ? Why measure performance similarity? What elements affect our perception of performance similarity?   Self-Similarity Matrices The concept of self-similarity matrices is fundamental for capturing structural properties of music recordings. Generally, one starts with a feature space L containing the elements of the feature sequence under consideration as well as with a similarity measure $f : \\mathcal{L} \\times \\mathcal{L} \\to \\mathbb{R}$.\n Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Interval Vectors  Definition An interval vector is an array of natural numbers which summarize the intervals present in a set of pitch classes. There is an equivalence between complementary intervals within the octave for example $2^{nd}$ minor and $7^{nth}$ major intervals, etc.  Examples :\n Any Major chord : $ [0, 0, 1, 1, 1, 0 ] $ Any Minor chord : $ [0, 0, 1, 1, 1, 0 ] $   Interval Vectors and Cadences    Typical Cadences      $\\textrm{V}$ \u0026amp; - \u0026amp; $\\textrm{I}$ authentic   $\\textrm{I}$ \u0026amp; - \u0026amp; $\\textrm{V}$ half   $\\textrm{IV}$ \u0026amp; - \u0026amp; $\\textrm{I}$ plagal   $\\textrm{V}$ \u0026amp; - \u0026amp; $\\textrm{VI}$ deceptive    Interval Vectors are commutative, i.e. $\\textrm{V} \\to \\textrm{I} = \\textrm{I} \\to \\textrm{V} $:\n $ (\\textrm{V/I})_{maj} = [1, 2, 2, 2, 3, 0] $ $ (\\textrm{V/I})_{min} = [2, 1, 2, 3, 2, 0] $   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  For every window $w_i^j$ we compute the interval vector of all notes in thye window/ Therefore, $\\textrm{Int_Vec}(w_i^j)\\in \\mathbb{N}^6$. $$ \\mathcal(W) = \\sum_{i=1}^N\\sum_{j=1}^\\nu \\textrm{Int_Vec}(w_i^j) $$ Then, let $\\mathcal{X}$ be some matrix decomposition of $\\mathcal{W}$. The SSM $\\mathcal{S}$ of $\\mathcal{X}$ is: $$ \\mathcal{S} = \\frac{\\mathcal{X} \\cdot \\mathcal{X}}{| \\mathcal{X} |^2} $$\n Test \\begin{tikzpicture} \\matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em] { F \u0026amp; B \\ \u0026amp; A \\}; \\path[-stealth] (m-1-1) edge node [above] {$\\beta$} (m-1-2) (m-1-2) edge node [right] {$\\rho$} (m-2-2) (m-1-1) edge node [left] {$\\alpha$} (m-2-2); \\end{tikzpicture}\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1612483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612483200,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://emmanouil-karystinaios.github.io/slides/example/","publishdate":"2021-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"On Performance Similarity and Structure Segmentation.","tags":"Department of Computational Perception","title":"Slides","type":"slides"},{"authors":["Emmanouil Karystinaios"],"categories":["Symbolic Representations"],"content":"Harmonic Trajectories in the Tonnetz Introduction The Neo-Riemannian Tonnetz is a way of visualizing musical relationships between chords. It was developed by music theorists to help understand how chords can transition smoothly from one to another.\nImagine a grid or network of interconnected points. Each point represents a different chord. The horizontal lines connect chords that are closely related, while the vertical lines connect chords that share similar tonal qualities.\nThe Tonnetz is based on the idea that chords can be transformed or changed into one another through small movements. These transformations are represented by diagonal lines on the Tonnetz. For example, a chord can be transformed into another chord by changing one note at a time, moving in a specific direction on the grid.\nBy studying the Tonnetz, musicians and theorists can analyze chord progressions and see how different chords are related to each other. It provides a visual representation of the harmonic possibilities and helps to explain the underlying structure of music.\nIn simple terms, the Neo-Riemannian Tonnetz is a grid that shows how chords in music are connected and can be transformed smoothly from one to another. It helps musicians and theorists understand how chords fit together and how they can create pleasing transitions in music.\nOverview PLR operations, also known as Parallel, Leading Tone, and Relative operations, are a set of chord transformations used in music theory. These operations help musicians understand how chords can be changed while maintaining a similar musical function.\n  Parallel operation: This operation involves changing a chord to another chord with the same root note but a different quality. For example, if you have a major chord, you can transform it into a minor chord by lowering the third note. This maintains the same root note but gives a different emotional quality to the chord.\n  Leading Tone operation: The leading tone refers to the seventh note of a major scale, which is one step below the tonic note. In this operation, a chord is transformed by changing the root note to its leading tone. This creates a sense of tension and a desire to resolve back to the tonic chord. For example, if you have a C major chord, you can transform it into a B diminished chord by changing the root note to B.\n  Relative operation: This operation involves changing a chord to another chord that shares a similar tonal quality but has a different root note. For example, if you have a C major chord, you can transform it into an A minor chord by changing the root note to A. The relative operation allows for smooth transitions between chords that have a related sound.\n  These PLR operations provide musicians with tools to transform chords while maintaining certain musical characteristics. By understanding these operations, musicians can create interesting chord progressions, establish tension and resolution, and explore different harmonic possibilities in their music.\nThe PLR operations are the foundation of the Tonnetz.\nThese operations/ transformations are the principal transformations of the Neo-Riemmanian theory which was mainly conceived by David Lewin (1933–2003). The mirror process is relative to the chord intervalic relations not the position of the shape in the circle.\nPLR as Group Operations The PLR group acts simply transitively on the set {$ n_M,n=0…11$} $\\cup$ { $n_m,n=0…11 $} of the 24 major and minor triads, where $n_M$ (resp. $n_m$) represents a major (resp. minor) triad with root n in the usual semi-tone encoding of pitch classes.\nIt is isomorphic to the dihedral group D24 of order 24, and is generated by the following two transformations.\nThe transformation $L: \\mathbb{Z} _{24} \\to \\mathbb{Z} _{24}$ is called the leading-tone operation, and is such that:\n$$ L(n_M)=(n+4)_m \\textrm{ and the complementary } L(n_m)=(n+8)_M $$\nThe transformation $R: \\mathbb{Z} _{24} \\to \\mathbb{Z} _{24}$ is called the relative operation, and is such that:\n$$ R(n_M)=(n+9)_m \\textrm{ and similarly the complementary } $$\nThough not a generator, the operation $P=(RL)3R$, called the parallel operation, is often considered, and is such that $P(n_M)=n_m$\nTrajectories in the Tonnetz The trajectory is defined as a path $\\mathcal{X} $ in the Tonnetz $T$, i.e. an ordered list of positions in the space $T$.\nLet us investigate some basic scenarios for trajectory construction. Placing the first note in the Tonnetz has no bearing on the descriptors we ultimately compute, so we can simply pick an arbitrary position. Now we consider the case where we have to place two notes: one of them is placed as in the previous case, and the second one is placed according to a criterion depending on a distance measure. To this end, we define a function $dist: \\mathbb{Z} _{12} \\times \\mathbb{Z} _{12} \\to \\mathbb{N}$, which assigns to the pitch class representation of notes, $x$ and $y$, their distance according to a given Tonnetz as: \\begin{equation} …","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"23fe897eea83541d26eed6aa3f3d54e1","permalink":"https://emmanouil-karystinaios.github.io/post/tonnetz/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/tonnetz/","section":"post","summary":"Introducing the Tonnetz and related applications for composition and musical analysis.","tags":["Academic"],"title":"Harmonic Trajectories in the Tonnetz","type":"post"},{"authors":["Emmanouil Karystinaios","Corentin Guichaoua","Moreno Andreatta","Louis Bigo","Isabelle Bloch"],"categories":null,"content":"","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606694400,"objectID":"54140727d157e7f951d46ca0ca8143d2","permalink":"https://emmanouil-karystinaios.github.io/publication/2020_jim_tonnetz/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/publication/2020_jim_tonnetz/","section":"publication","summary":"In this article, we propose a new approach for stylistic music classification, driven by chord material. we present a novel approach without vocabulary restrictions on the chord material but rather an evaluation of the total harmonic content following closely Louis Bigo's apporach on trajectories in generic simplicial complexes.","tags":[],"title":"Music genre descriptor for classification based on Tonnetz trajectories","type":"publication"}]